# -*- coding: utf-8 -*-
"""SM64_Speedruns.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pw8yurN4mwKkEWOw_1p8bXouGArAK6O

#  **SM64 Speedruns - A Python Test**

\** **Ignore any redundant / unnecessary comments, they're mainly just notes for me because I am a noob :]**

Learning Python for data analysis / science and wanted to test what I've learned so far on a personal project using a dataset from Kaggle. (https://www.kaggle.com/code/mcpenguin/super-mario-64-speedruns-data-collection)
\
\
I also haven't used git in a while so this is also sort of a guinea pig for re-learning that.
"""

# Connecting to Google Drive to access dataset files saved there.
from google.colab import drive
drive.mount('/content/drive')

"""### **SQLite Connection**"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import seaborn as sns
import numpy as np
import csv, sqlite3, os
import openpyxl

# Connection Object to establish connection to a sqlite3 database.
connObj = sqlite3.connect('SPEEDRUNS.db')

# Loading the SQL Magic extension to be able to run SQL queries through / within Python code (available via Pandas)
# %load_ext sql
# %sql sqlite:///SPEEDRUNS.db

# Cursor Object against connObj to access controller functions like fetchall(), open(), execute(), close(), etc.
# Don't really need this I think since I'm using SQL Magic, but it may be useful for later.
cursorObj = connObj.cursor()

"""### **Merging Separate .CSV Files Into A Single Pandas DataFrame.**

**Gathering Datasets (.CSV)**

Included the separate read_csv() calls for each dataset .csv file. I was originally going to do this, and then thought of merging them in VBA for fun, but then wanted to do it in Python.
"""

# Assigning the datasets location for SM64 Speedruns to a variable called "repo"
repo = r'/content/drive/MyDrive/Kaggle/Datasets/SM64 Speedruns/'

# Lists files within the specified directory, in this case "repo".
files_in_repo = os.listdir(repo)

# Found online. Didn't really think I would need this, but remembered that I do have a .xlsm file in this directory so this is for fun.
csv_files = [f for f in files_in_repo if f.endswith('.csv')] # This checks to see if each file within the repo's directory ends in '.csv'.

zeroStar = pd.read_csv('/content/drive/MyDrive/Kaggle/Datasets/SM64 Speedruns/data_0 Star.csv')
oneStar = pd.read_csv('/content/drive/MyDrive/Kaggle/Datasets/SM64 Speedruns/data_1 Star.csv')
sixteenStar = pd.read_csv('/content/drive/MyDrive/Kaggle/Datasets/SM64 Speedruns/data_16 Star.csv')
seventyStar = pd.read_csv('/content/drive/MyDrive/Kaggle/Datasets/SM64 Speedruns/data_70 Star.csv')
oneTwentyStar = pd.read_csv('/content/drive/MyDrive/Kaggle/Datasets/SM64 Speedruns/data_120 Star.csv')

# List to hold the list of dataframes / csv files.
df_list = []

"""---

**Appending Separate .CSV DataFrames to the "df_list" List via For Loop**

Found this online because I wasn't sure of the syntax on doing the loop, and the error handling is good, too.

It all makes sense though - here's my walkthrough:

1.   Looping through the list of `csv_files` within `repo`, assigning each to "`csv`".

2.   The path to the file is created by joining the `repo` path and the .csv filename.

1.   Creating a DataFrame (for each iteration of `csv_files`) using the `read_csv()` function and the `file_path` variable.
2.   The DataFrame is appended to the DataFrame List `df_list`.

1.   `try` / `except` = error handling on the encoding types for the files.
"""

for csv in csv_files:
    file_path = os.path.join(repo, csv)
    try:
        # Try reading the file using default UTF-8 encoding
        df = pd.read_csv(file_path)
        df_list.append(df)
    except UnicodeDecodeError:
        try:
            # If UTF-8 fails, try reading the file using UTF-16 encoding with tab separator
            df = pd.read_csv(file_path, sep='\t', encoding='utf-16')
            df_list.append(df)
        except Exception as e:
            print(f"Could not read file {csv} because of error: {e}")
    except Exception as e:
        print(f"Could not read file {csv} because of error: {e}")

"""---

**Concatenating the DataFrames and Saving to a Single .CSV File**
"""

# Concat all data into a single DataFrame

complete_df = pd.concat(df_list, ignore_index=True)

"""---

### **Cleansing / Restructuring Data With *NumPy* And *Pandas***
"""

full_path_to_dataset = os.path.join(repo, 'ALL_CATEGORIES.csv')

# Save the final result to a new .csv file (appears in the G Drive folder after 15-30 sec).
complete_df.to_csv(full_path_to_dataset, index=False)

allCategoriesDF = pd.read_csv(full_path_to_dataset)
# Loading the dataframe into the SPEEDRUNS database using the connObj. Specified a table name of "ALL_CAT_SPEEDRUNS".
allCategoriesDF.to_sql('ALL_CAT_SPEEDRUNS', connObj, if_exists='replace', index=False)

allCategoriesDF

# Dropping some unwanted columns.
#
# Note: I'll be remaking the 'id' column since multiple datasets were merged.
cols_to_drop = ['Unnamed: 0',
                  'id',
                  'player_id']

allCategoriesDF.drop(cols_to_drop, inplace=True, axis=1)

sorted(allCategoriesDF.columns)